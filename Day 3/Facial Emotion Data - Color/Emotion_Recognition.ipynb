{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Dataset - 'https://www.kaggle.com/datasets/sanidhyak/human-face-emotions/data'"
      ],
      "metadata": {
        "id": "J9Lt706PUIK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEZZd5fvMKhB"
      },
      "outputs": [],
      "source": [
        "!pip install split-folders\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "\n",
        "import splitfolders\n"
      ],
      "metadata": {
        "id": "54tzTNL_PZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR = \"/kaggle/input/human-face-emotions/data\"\n"
      ],
      "metadata": {
        "id": "uwRtGjVGPZLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"/kaggle/working/emotion_split\"\n",
        "\n",
        "# Split: 70% train, 15% val, 15% test\n",
        "splitfolders.ratio(\n",
        "    INPUT_DIR,\n",
        "    output=OUTPUT_DIR,\n",
        "    seed=42,\n",
        "    ratio=(0.7, 0.15, 0.15),\n",
        "    group_prefix=None\n",
        ")\n",
        "\n",
        "print(\"✅ Dataset split completed!\")\n"
      ],
      "metadata": {
        "id": "2YVpvT6oPZI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
        "VAL_DIR   = os.path.join(OUTPUT_DIR, \"val\")\n",
        "TEST_DIR  = os.path.join(OUTPUT_DIR, \"test\")\n",
        "\n",
        "emotion_classes = [\n",
        "    d for d in os.listdir(TRAIN_DIR)\n",
        "    if os.path.isdir(os.path.join(TRAIN_DIR, d))\n",
        "]\n",
        "\n",
        "print(\"Emotion Classes:\", emotion_classes)\n"
      ],
      "metadata": {
        "id": "4c1qy9aWPZF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224      # ✅ As you requested\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "K6UwnWdYPZDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",              # ✅ COLOR\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "mT8QkdcjPZAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(train_generator.class_indices.keys())\n",
        "NUM_CLASSES = train_generator.num_classes\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of classes:\", NUM_CLASSES)\n",
        "print(\"Class indices:\", train_generator.class_indices)\n"
      ],
      "metadata": {
        "id": "LYSbt6ZvPY-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_generator:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "\n",
        "        img = images[i]\n",
        "        plt.imshow(img)\n",
        "\n",
        "        true_index = int(labels[i])\n",
        "        true_label = class_names[true_index]\n",
        "\n",
        "        plt.title(true_label)\n",
        "        plt.axis(\"off\")\n",
        "    break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WjRzkRXEPY74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(224, 224, 3)),   # ✅ 224 RGB input\n",
        "\n",
        "    Conv2D(32, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(256, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(256, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "OrQ_ER0rPY5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30\n",
        ")\n"
      ],
      "metadata": {
        "id": "pNxH7SvWP0P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "UrT094dwP0Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"emotion_cnn_rgb_224.h5\")\n",
        "print(\"✅ 224×224 RGB emotion model saved!\")\n"
      ],
      "metadata": {
        "id": "QCDKD_nYP0KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & Import Libraries\n",
        "\n",
        "!pip install split-folders\n"
      ],
      "metadata": {
        "id": "mre7rqjyP0HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "import splitfolders\n"
      ],
      "metadata": {
        "id": "wl4SfRRBP0Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset into Train / Val / Test\n",
        "\n",
        "INPUT_DIR = \"/kaggle/input/human-face-emotions/data\"   # change to your path\n",
        "OUTPUT_DIR = \"/kaggle/working/emotion_split\"\n",
        "\n",
        "splitfolders.ratio(\n",
        "    INPUT_DIR,\n",
        "    output=OUTPUT_DIR,\n",
        "    seed=42,\n",
        "    ratio=(0.7, 0.15, 0.15)\n",
        ")\n",
        "\n",
        "print(\"✅ Dataset split completed!\")\n"
      ],
      "metadata": {
        "id": "6-C0m3nfP0Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Paths & Load Classes\n",
        "\n",
        "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
        "VAL_DIR   = os.path.join(OUTPUT_DIR, \"val\")\n",
        "TEST_DIR  = os.path.join(OUTPUT_DIR, \"test\")\n",
        "\n",
        "emotion_classes = [\n",
        "    d for d in os.listdir(TRAIN_DIR)\n",
        "    if os.path.isdir(os.path.join(TRAIN_DIR, d))\n",
        "]\n",
        "\n",
        "print(\"Emotion Classes:\", emotion_classes)\n"
      ],
      "metadata": {
        "id": "G7KXyXWcRJ8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generators (rescale = 1./255)\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "j56lfeNcRPb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    classes=emotion_classes,\n",
        "    class_mode=\"sparse\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "AkB3-TI6RPYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(train_generator.class_indices.keys())\n",
        "NUM_CLASSES = train_generator.num_classes\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of classes:\", NUM_CLASSES)\n",
        "print(\"Class indices:\", train_generator.class_indices)\n"
      ],
      "metadata": {
        "id": "W4NGSfsERPWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Training Samples (RGB)\n",
        "\n",
        "for images, labels in train_generator:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "\n",
        "        img = images[i]\n",
        "        plt.imshow(img)\n",
        "\n",
        "        true_index = int(labels[i])\n",
        "        true_label = class_names[true_index]\n",
        "\n",
        "        plt.title(true_label)\n",
        "        plt.axis(\"off\")\n",
        "    break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lDVNiHB-RPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build VGG16 Model (rescale version)\n",
        "\n",
        "base_model = VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "# Freeze all VGG16 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "4sGI7TuYRPQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "outputs = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "zbAAwAR-RPOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20\n",
        ")\n"
      ],
      "metadata": {
        "id": "VX26p-wVRPLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Accuracy & Loss\n",
        "\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(acc, label=\"Train Accuracy\")\n",
        "plt.plot(val_acc, label=\"Val Accuracy\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss, label=\"Train Loss\")\n",
        "plt.plot(val_loss, label=\"Val Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e-Qp_pPIRbMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Test Set\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "g_7yfEPYRbJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict ONE Validation Image (Like Your Earlier Code)\n",
        "\n",
        "for images, labels in val_generator:\n",
        "    img = images[0]\n",
        "\n",
        "    true_index = int(labels[0])\n",
        "    true_label = class_names[true_index]\n",
        "\n",
        "    preds = model.predict(np.expand_dims(img, axis=0), verbose=0)[0]\n",
        "    pred_index = np.argmax(preds)\n",
        "    pred_label = class_names[pred_index]\n",
        "    confidence = preds[pred_index] * 100\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\n",
        "        f\"Actual: {true_label}\\n\"\n",
        "        f\"Predicted: {pred_label}\\n\"\n",
        "        f\"Confidence: {confidence:.2f}%\"\n",
        "    )\n",
        "    break\n"
      ],
      "metadata": {
        "id": "pNkNuSZERbHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple Predictions Visualization (3×3 Grid\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "for images, labels in val_generator:\n",
        "    for i in range(9):\n",
        "        img = images[i]\n",
        "\n",
        "        true_index = int(labels[i])\n",
        "        true_label = class_names[true_index]\n",
        "\n",
        "        preds = model.predict(np.expand_dims(img, axis=0), verbose=0)[0]\n",
        "        pred_index = np.argmax(preds)\n",
        "        pred_label = class_names[pred_index]\n",
        "        confidence = preds[pred_index] * 100\n",
        "\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\n",
        "            f\"A: {true_label}\\n\"\n",
        "            f\"P: {pred_label}\\n\"\n",
        "            f\"Conf: {confidence:.2f}%\",\n",
        "            fontsize=9\n",
        "        )\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AwzBC1m0RbEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Model\n",
        "\n",
        "model.save(\"emotion_vgg16_sparse_model.h5\")\n",
        "print(\"✅ VGG16 emotion model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "CITVpHTTRbCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Webcam\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ==========================\n",
        "# Load Haar Cascade\n",
        "# ==========================\n",
        "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# ==========================\n",
        "# Load Trained VGG16 Model\n",
        "# ==========================\n",
        "IMG_SIZE = 224\n",
        "MODEL_PATH = \"emotion_vgg16_sparse_model.h5\"   # your VGG16 model\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"✅ VGG16 emotion model loaded\")\n",
        "\n",
        "# ==========================\n",
        "# Class Names (MUST match training order!)\n",
        "# ==========================\n",
        "class_names = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "# ==========================\n",
        "# Start Webcam\n",
        "# ==========================\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # --------------------\n",
        "    # Convert full frame to grayscale for face detection only\n",
        "    # --------------------\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # --------------------\n",
        "    # Detect faces on grayscale frame\n",
        "    # --------------------\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(80, 80)\n",
        "    )\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        # --------------------\n",
        "        # Crop face (COLOR, BGR)\n",
        "        # --------------------\n",
        "        face_color = frame[y:y+h, x:x+w]   # BGR crop\n",
        "\n",
        "        if face_color.size == 0:\n",
        "            continue\n",
        "\n",
        "        # --------------------\n",
        "        # Preprocess for VGG16 (rescale = 1./255 version)\n",
        "        # --------------------\n",
        "        # Resize\n",
        "        face = cv2.resize(face_color, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        # Convert BGR -> RGB\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)   # (224, 224, 3)\n",
        "\n",
        "        # Normalize (SAME as training)\n",
        "        face_array = face.astype(\"float32\") / 255.0\n",
        "\n",
        "        # Add batch dimension -> (1, 224, 224, 3)\n",
        "        face_array = np.expand_dims(face_array, axis=0)\n",
        "\n",
        "        # --------------------\n",
        "        # Predict Emotion\n",
        "        # --------------------\n",
        "        preds = model.predict(face_array, verbose=0)[0]\n",
        "\n",
        "        \"\"\"\n",
        "        preds shape = (NUM_CLASSES,)\n",
        "        example -> [0.02, 0.01, 0.05, 0.78, 0.04, 0.06, 0.04]\n",
        "\n",
        "        pred_index = argmax(preds)\n",
        "        confidence = preds[pred_index] * 100\n",
        "        label = class_names[pred_index]\n",
        "        \"\"\"\n",
        "\n",
        "        pred_index = np.argmax(preds)\n",
        "        confidence = preds[pred_index] * 100\n",
        "        label = class_names[pred_index]\n",
        "\n",
        "        text = f\"{label} ({confidence:.2f}%)\"\n",
        "\n",
        "        # --------------------\n",
        "        # Draw Results\n",
        "        # --------------------\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            text,\n",
        "            (x, y - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.8,\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "    # --------------------\n",
        "    # Show Frame\n",
        "    # --------------------\n",
        "    cv2.imshow(\"Emotion Detection (VGG16 RGB)\", frame)\n",
        "\n",
        "    # Press ESC to exit\n",
        "    if cv2.waitKey(1) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "KjifZ4nKRa_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}