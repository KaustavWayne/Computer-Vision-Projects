{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swoFiTu5Isl6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRq6iREPRBeS"
      },
      "outputs": [],
      "source": [
        "TRAIN_DIR = '/kaggle/input/face-expression-recognition-dataset/images/train'\n",
        "\n",
        "emotion_classes = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
        "\n",
        "print(f'Emotion Classes:', emotion_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6L4f91hRBbG"
      },
      "outputs": [],
      "source": [
        "emotion_classes = ['surprise', 'fear', 'angry', 'neutral', 'sad', 'disgust', 'happy']\n",
        "\n",
        "# Data Augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range=15,\n",
        "    #shear_range=0.0,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# Data Generators\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/kaggle/input/face-expression-recognition-dataset/images/train',\n",
        "    target_size = (48, 48),\n",
        "    color_mode = 'grayscale',  # If rgb use this\n",
        "    batch_size = 32,\n",
        "    shuffle=True,\n",
        "    classes=emotion_classes,\n",
        "    class_mode= 'categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/kaggle/input/face-expression-recognition-dataset/images/validation',\n",
        "    target_size = (48, 48),\n",
        "    color_mode = 'grayscale',  # If rgb use this\n",
        "    batch_size = 32,\n",
        "    shuffle=False,\n",
        "    classes=emotion_classes,\n",
        "    class_mode= 'categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you use preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSYsqvIGRBWC"
      },
      "outputs": [],
      "source": [
        "class_names = list(train_generator.class_indices.keys())\n",
        "NUM_CLASSES = train_generator.num_classes\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of classes:\", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btXFVsogVMZG"
      },
      "outputs": [],
      "source": [
        "print(train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBPbpd1ARBTL"
      },
      "outputs": [],
      "source": [
        "# Visualize Sample Images\n",
        "\n",
        "for images, labels in train_generator:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        img = images[i]\n",
        "        plt.imshow(img, cmap = 'gray')  # if rgb no cmap needed\n",
        "\n",
        "        true_index = np.argmax(labels[i])\n",
        "        true_label = class_names[true_index]\n",
        "        plt.title(true_label)\n",
        "        plt.axis('off')\n",
        "    break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXG1o6yTVpCK"
      },
      "outputs": [],
      "source": [
        "# IF the image is RGB then you can use transfer learning\n",
        "# Build VGG16 Emotion Model\n",
        "\n",
        "# base_model = VGG16(\n",
        "#     weights=\"imagenet\",\n",
        "#     include_top=False,\n",
        "#     input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "# )\n",
        "\n",
        "# # Freeze base model\n",
        "# base_model.trainable = False\n",
        "\n",
        "# x = base_model.output\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(256, activation=\"relu\")(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "\n",
        "# outputs = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "# model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=\"adam\",\n",
        "#     loss=\"categorical_crossentropy\",\n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j534uP2eRBQG"
      },
      "outputs": [],
      "source": [
        "# Build CNN Model\n",
        "\n",
        "model = Sequential([\n",
        "\n",
        "    Conv2D(32, kernel_size= (3,3), activation='relu', input_shape = (48, 48, 1)),  # if rgb - then inputshape = (48,48, 3)\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, kernel_size= (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, kernel_size= (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense Layers\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJLQ9waqRBLT"
      },
      "outputs": [],
      "source": [
        "# Train the Model\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "# history = cnn_model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     epochs=25\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGFV9i_KRBIs"
      },
      "outputs": [],
      "source": [
        "# Plot Accuracy & Loss\n",
        "\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(acc, label=\"Train Accuracy\")\n",
        "plt.plot(val_acc, label=\"Val Accuracy\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(loss, label=\"Train Loss\")\n",
        "plt.plot(val_loss, label=\"Val Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BisQl-jGRBFz"
      },
      "outputs": [],
      "source": [
        "# Evaluate on Test Set\n",
        "\n",
        "# test_loss, test_acc = model.evaluate(test_generator)\n",
        "# print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoqEXN58RBC_"
      },
      "outputs": [],
      "source": [
        "# Predict ONE random test image (Correct for Emotion)\n",
        "\n",
        "#for images, labels in test_generator:\n",
        "for images, labels in val_generator:\n",
        "    img = images[0]\n",
        "\n",
        "    # True label (categorical)\n",
        "    true_index = np.argmax(labels[0])      # âœ… one-hot â†’ class index\n",
        "    true_label = class_names[true_index]\n",
        "\n",
        "    # Prediction\n",
        "    preds = model.predict(np.expand_dims(img, axis=0), verbose=0)[0]\n",
        "    # preds shape = (NUM_CLASSES,)\n",
        "\n",
        "    pred_index = np.argmax(preds)          # âœ… softmax â†’ class index\n",
        "    pred_label = class_names[pred_index]\n",
        "    confidence = preds[pred_index] * 100\n",
        "\n",
        "    # Show\n",
        "    plt.imshow(img)\n",
        "    plt.title(\n",
        "        f\"Actual: {true_label}\\n\"\n",
        "        f\"Predicted: {pred_label}\\n\"\n",
        "        f\"Confidence: {confidence:.2f}%\"\n",
        "    )\n",
        "    plt.axis(\"off\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7kWVYTpRBBK"
      },
      "outputs": [],
      "source": [
        "# Predict Random Test Images\n",
        "\n",
        "#for images, labels in test_generator:\n",
        "for images, labels in val_generator:\n",
        "    plt.figure(figsize=(8,8))\n",
        "    for i in range(9):\n",
        "        img = images[i]\n",
        "        true_index = np.argmax(labels[i])  # categorical label\n",
        "        true_label = class_names[true_index]\n",
        "\n",
        "        preds = model.predict(np.expand_dims(img, axis=0), verbose=0)[0]  # preds shape = (NUM_CLASSES,)\n",
        "        pred_index = np.argmax(preds)\n",
        "        pred_label = class_names[pred_index]\n",
        "        confidence = preds[pred_index] * 100\n",
        "\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.imshow(img, cmap = 'gray') # if rgb no gray\n",
        "        plt.title(f\"A: {true_label}\\nP: {pred_label}\\n{confidence:.2f}%\")\n",
        "        plt.axis(\"off\")\n",
        "    break\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4cN1HAuRA-6"
      },
      "outputs": [],
      "source": [
        "# Save Model\n",
        "\n",
        "model.save(\"emotion_vgg16_model.h5\")\n",
        "print(\"âœ… Model saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZgZbfNFRA7R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = \"/content/test_emotion.jpg\"   # change path\n",
        "\n",
        "# Load & preprocess\n",
        "img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE), color_mode=\"grayscale\") # grey scale\n",
        "# img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE)) # color mode rgb\n",
        "img_array = image.img_to_array(img) / 255.0  # shape = (H, W, 3)\n",
        "img_array = np.expand_dims(img_array, axis=0) # shape = (1, H, W, 3)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "pred_index = np.argmax(preds)\n",
        "predicted_label = class_names[pred_index]\n",
        "confidence = preds[pred_index] * 100\n",
        "\n",
        "# Show result\n",
        "plt.imshow(img, cmap='gray') # for grey otherwise not needed grey if rgb images is there\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Predicted: {predicted_label}\\nConfidence: {confidence:.2f}%\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DOUR104Z5i5"
      },
      "outputs": [],
      "source": [
        "# CASE 1 â€” Using VGG16 Model (RGB, 224Ã—224)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = \"/content/test_emotion.jpg\"   # change path\n",
        "\n",
        "# Load image using cv2 (BGR)\n",
        "img_bgr = cv2.imread(img_path)\n",
        "\n",
        "# Resize\n",
        "img_bgr = cv2.resize(img_bgr, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "# Convert BGR -> RGB\n",
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Normalize\n",
        "img_array = img_rgb.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add batch dimension\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "pred_index = np.argmax(preds)\n",
        "predicted_label = class_names[pred_index]\n",
        "confidence = preds[pred_index] * 100\n",
        "\n",
        "# Show result\n",
        "plt.imshow(img_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Predicted: {predicted_label}\\nConfidence: {confidence:.2f}%\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using preprocess_input in ImageDataGenerator\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "img_path = \"/content/test_emotion.jpg\"   # change path\n",
        "\n",
        "# Load image using cv2 (BGR)\n",
        "img_bgr = cv2.imread(img_path)\n",
        "\n",
        "# Resize\n",
        "img_bgr = cv2.resize(img_bgr, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "# Convert BGR -> RGB (optional but good for display)\n",
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Add batch dimension first\n",
        "img_array = np.expand_dims(img_rgb, axis=0)\n",
        "\n",
        "# ðŸ”¥ VGG16 preprocessing (REPLACES /255 normalization)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "pred_index = np.argmax(preds)\n",
        "predicted_label = class_names[pred_index]\n",
        "confidence = preds[pred_index] * 100\n",
        "\n",
        "# Show result\n",
        "plt.imshow(img_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Predicted: {predicted_label}\\nConfidence: {confidence:.2f}%\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbdbGS2zaAOG"
      },
      "outputs": [],
      "source": [
        "# CASE 2 â€” Using Custom CNN with Grayscale (48Ã—48Ã—1)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = \"/content/test_emotion.jpg\"   # change path\n",
        "\n",
        "# Load image in grayscale\n",
        "img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Resize\n",
        "img_gray = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "# Normalize\n",
        "img_array = img_gray.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add channel dimension (H, W, 1)\n",
        "img_array = np.expand_dims(img_array, axis=-1)\n",
        "\n",
        "# Add batch dimension (1, H, W, 1)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict\n",
        "preds = cnn_model.predict(img_array, verbose=0)[0]\n",
        "\n",
        "pred_index = np.argmax(preds)\n",
        "predicted_label = class_names[pred_index]\n",
        "confidence = preds[pred_index] * 100\n",
        "\n",
        "# Show result\n",
        "plt.imshow(img_gray, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Predicted: {predicted_label}\\nConfidence: {confidence:.2f}%\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Binary vs Multi-Class (Very Important Table)\n",
        "\n",
        "| Case        | Model Output                     | How to get confidence |\n",
        "| ----------- | -------------------------------- | --------------------- |\n",
        "| Binary      | Single scalar `prob = P(class1)` | `prob` or `1-prob`    |\n",
        "| Multi-class | Vector `[p0, p1, ..., pN]`       | `preds[pred_index]`   |\n",
        "\n",
        "- Multiclass means categorical, sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model Type                     | Output Shape     | Use `preds[pred_index]`? |\n",
        "| ------------------------------ | ---------------- | ------------------------ |\n",
        "| Binary (sigmoid, 1 unit)       | `(1,)` or scalar | âŒ NO                     |\n",
        "| Categorical (softmax, N units) | `(N,)`           | âœ… YES                    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Side-by-Side Comparison (Very Important)\n",
        "\n",
        "| Aspect           | Grayscale CNN     | Color VGG16      |\n",
        "| ---------------- | ----------------- | ---------------- |\n",
        "| Crop from        | `gray[...]`       | `frame[...]`     |\n",
        "| Channels         | 1                 | 3                |\n",
        "| Input shape      | `(H, W, 1)`       | `(H, W, 3)`      |\n",
        "| Color conversion | âŒ Not needed      | âœ… Required       |\n",
        "| Why              | Already grayscale | BGR â†’ RGB needed |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## If you train your custom CNN on colorful (RGB) images, then in webcam inference you MUST use:\n",
        "\n",
        "```\n",
        "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajn-36XxZOzr"
      },
      "outputs": [],
      "source": [
        "# For colorful images webcam code\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load Haar Cascade\n",
        "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Load trained model\n",
        "IMG_SIZE = 224\n",
        "MODEL_PATH = \"emotion_vgg16_sparse_model.h5\"\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"âœ… Emotion model loaded\")\n",
        "\n",
        "# Class names (must match training order!)\n",
        "class_names = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(80, 80)\n",
        "    )\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        face = frame[y:y+h, x:x+w]  # color crop (BGR)\n",
        "        if face.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Preprocess\n",
        "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # (H, W, 3)\n",
        "        face = face.astype(\"float32\") / 255.0\n",
        "        face = np.expand_dims(face, axis=0)  # (1, H, W, 3)\n",
        "\n",
        "        # Predict\n",
        "        preds = model.predict(face, verbose=0)[0]\n",
        "        pred_index = np.argmax(preds)\n",
        "        confidence = preds[pred_index] * 100\n",
        "        label = class_names[pred_index]\n",
        "\n",
        "        text = f\"{label} ({confidence:.2f}%)\"\n",
        "\n",
        "        # Draw\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            text,\n",
        "            (x, y - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.8,\n",
        "            (0,255,0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "    cv2.imshow(\"Emotion Detection (VGG16 + Sparse)\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For color ful images using preprocessor_input\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "\n",
        "# Load Haar Cascade\n",
        "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Load trained model\n",
        "IMG_SIZE = 224\n",
        "MODEL_PATH = \"emotion_vgg16_sparse_model.h5\"\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"âœ… Emotion model loaded\")\n",
        "\n",
        "# Class names (must match training order!)\n",
        "class_names = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(80, 80)\n",
        "    )\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        face = frame[y:y+h, x:x+w]  # color crop (BGR)\n",
        "        if face.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Preprocess\n",
        "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)   # (H, W, 3)\n",
        "\n",
        "        # Add batch dimension first\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "\n",
        "        # ðŸ”¥ VGG16 preprocessing (REPLACES /255)\n",
        "        face = preprocess_input(face)\n",
        "\n",
        "\n",
        "        # Predict\n",
        "        preds = model.predict(face, verbose=0)[0]\n",
        "        pred_index = np.argmax(preds)\n",
        "        confidence = preds[pred_index] * 100\n",
        "        label = class_names[pred_index]\n",
        "\n",
        "        text = f\"{label} ({confidence:.2f}%)\"\n",
        "\n",
        "        # Draw\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            text,\n",
        "            (x, y - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.8,\n",
        "            (0,255,0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "    cv2.imshow(\"Emotion Detection (VGG16 + Sparse)\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cJbVHlPdfBk"
      },
      "outputs": [],
      "source": [
        "# For grey scale images webcam code\n",
        "\n",
        "# WEBCAM CODE â€” CUSTOM CNN (GRAYSCALE, SPARSE)\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ==========================\n",
        "# Load Haar Cascade\n",
        "# ==========================\n",
        "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# ==========================\n",
        "# Load Trained CNN Model\n",
        "# ==========================\n",
        "IMG_SIZE = 48   # must match training\n",
        "MODEL_PATH = \"emotion_cnn_sparse_model.h5\"\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "print(\"âœ… Grayscale CNN emotion model loaded\")\n",
        "\n",
        "# ==========================\n",
        "# Class Names (MUST match training order)\n",
        "# ==========================\n",
        "class_names = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "# ==========================\n",
        "# Start Webcam\n",
        "# ==========================\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert full frame to grayscale (for face detection + model)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces on grayscale frame\n",
        "    faces = face_cascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(80, 80)\n",
        "    )\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "\n",
        "        # --------------------\n",
        "        # Crop face (grayscale)\n",
        "        # --------------------\n",
        "        face_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "        if face_gray.size == 0:\n",
        "            continue\n",
        "\n",
        "        # --------------------\n",
        "        # Preprocess for CNN\n",
        "        # --------------------\n",
        "        face_gray = cv2.resize(face_gray, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        # Normalize\n",
        "        face_array = face_gray.astype(\"float32\") / 255.0\n",
        "\n",
        "        # Add channel dimension -> (H, W, 1)\n",
        "        face_array = np.expand_dims(face_array, axis=-1)\n",
        "\n",
        "        # Add batch dimension -> (1, H, W, 1)\n",
        "        face_array = np.expand_dims(face_array, axis=0)\n",
        "\n",
        "        # --------------------\n",
        "        # Predict Emotion\n",
        "        # --------------------\n",
        "        preds = model.predict(face_array, verbose=0)[0]\n",
        "        pred_index = np.argmax(preds)\n",
        "        confidence = preds[pred_index] * 100\n",
        "        label = class_names[pred_index]\n",
        "\n",
        "        text = f\"{label} ({confidence:.2f}%)\"\n",
        "\n",
        "        # --------------------\n",
        "        # Draw Results\n",
        "        # --------------------\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            text,\n",
        "            (x, y - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.8,\n",
        "            (0, 255, 0),\n",
        "            2\n",
        "        )\n",
        "\n",
        "    cv2.imshow(\"Emotion Detection (CNN Grayscale)\", frame)\n",
        "\n",
        "    # ESC to exit\n",
        "    if cv2.waitKey(1) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRUA2bUUsQck"
      },
      "outputs": [],
      "source": [
        "# Extra\n",
        "\n",
        "# from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# train_gen = ImageDataGenerator(\n",
        "#     preprocessing_function=preprocess_input,\n",
        "#     rotation_range=15,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     zoom_range=0.1,\n",
        "#     horizontal_flip=True\n",
        "# )\n",
        "\n",
        "# val_gen = ImageDataGenerator(\n",
        "#     preprocessing_function=preprocess_input\n",
        "# )\n",
        "\n",
        "# train_data = train_gen.flow_from_directory(\n",
        "#     train_path,\n",
        "#     target_size=(224,224),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     color_mode='rgb',\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# val_data = val_gen.flow_from_directory(\n",
        "#     validation_path,\n",
        "#     target_size=(224,224),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     color_mode='rgb',\n",
        "#     shuffle=False\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
